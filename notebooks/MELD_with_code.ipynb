{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MELD_with_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdhrpMPAEQOLVbtkfghLrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae898/MELD/blob/master/notebooks/MELD_with_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKvTpsj7WWfj"
      },
      "source": [
        "This notebook closely follows [the original repo](https://github.com/declare-lab/MELD) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnIWWiRT8yhg"
      },
      "source": [
        "# Download the pre-computed features and models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqTYE5HU81Qc",
        "outputId": "06895d6f-652e-4944-ca26-24a4cde46778",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "# wget to download\n",
        "!wget http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Features.Models.tar.gz\n",
        "# untar\n",
        "!tar -zxvf MELD.Features.Models.tar.gz\n",
        "# remove the tar\n",
        "!rm -rf MELD.Features.Models.tar.gz\n",
        "\n",
        "# move the features and the models to the current directory\n",
        "!mv MELD.Features.Models/features/ ./features\n",
        "!mv MELD.Features.Models/models/ ./models"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 11:28:17--  http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Features.Models.tar.gz\n",
            "Resolving web.eecs.umich.edu (web.eecs.umich.edu)... 141.212.113.214\n",
            "Connecting to web.eecs.umich.edu (web.eecs.umich.edu)|141.212.113.214|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 922694594 (880M) [application/x-gzip]\n",
            "Saving to: ‘MELD.Features.Models.tar.gz’\n",
            "\n",
            "MELD.Features.Model 100%[===================>] 879.95M  52.5MB/s    in 18s     \n",
            "\n",
            "2020-10-11 11:28:36 (47.7 MB/s) - ‘MELD.Features.Models.tar.gz’ saved [922694594/922694594]\n",
            "\n",
            "MELD.Features.Models/\n",
            "MELD.Features.Models/models/\n",
            "MELD.Features.Models/models/audio_weights_emotion.hdf5\n",
            "MELD.Features.Models/models/text_weights_emotion.hdf5\n",
            "MELD.Features.Models/models/bimodal_weights_emotion.hdf5\n",
            "MELD.Features.Models/models/text_weights_sentiment.hdf5\n",
            "MELD.Features.Models/models/audio_weights_sentiment.hdf5\n",
            "MELD.Features.Models/models/bimodal_weights_sentiment.hdf5\n",
            "MELD.Features.Models/features/\n",
            "MELD.Features.Models/features/text_glove_CNN_sentiment.pkl\n",
            "MELD.Features.Models/features/text_glove_CNN_emotion.pkl\n",
            "MELD.Features.Models/features/text_emotion.pkl\n",
            "MELD.Features.Models/features/bimodal_sentiment.pkl\n",
            "MELD.Features.Models/features/text_glove_average_emotion.pkl\n",
            "MELD.Features.Models/features/audio_embeddings_feature_selection_sentiment.pkl\n",
            "MELD.Features.Models/features/data_sentiment.p\n",
            "MELD.Features.Models/features/data_emotion.p\n",
            "MELD.Features.Models/features/text_glove_average_sentiment.pkl\n",
            "MELD.Features.Models/features/audio_sentiment.pkl\n",
            "MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl\n",
            "MELD.Features.Models/features/text_sentiment.pkl\n",
            "MELD.Features.Models/features/audio_emotion.pkl\n",
            "MELD.Features.Models/README.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdLIsLjpWQ2c"
      },
      "source": [
        "# Import the necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEBVuv6FWPFO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "from collections import Counter, defaultdict\n",
        "import argparse\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRbZy8fwWjOj"
      },
      "source": [
        "# Define the Dataloader class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVhDLg-FQE58"
      },
      "source": [
        "class Dataloader:\n",
        "\n",
        "    def __init__(self, mode=None):\n",
        "\n",
        "        try:\n",
        "            assert(mode is not None)\n",
        "        except AssertionError as e:\n",
        "            print(\"Set mode as 'Sentiment' or 'Emotion'\")\n",
        "            exit()\n",
        "\n",
        "        self.MODE = mode  # Sentiment or Emotion classification mode\n",
        "        self.max_l = 50  # Maximum length of the sentence\n",
        "\n",
        "        \"\"\"\n",
        "            Loading the dataset: \n",
        "                - revs is a dictionary with keys/value: \n",
        "                    - text: original sentence\n",
        "                    - split: train/val/test :: denotes the which split the tuple belongs to\n",
        "                    - y: label of the sentence\n",
        "                    - dialog: ID of the dialog the utterance belongs to\n",
        "                    - utterance: utterance number of the dialog ID\n",
        "                    - num_words: number of words in the utterance\n",
        "                - W: glove embedding matrix\n",
        "                - vocab: the vocabulary of the dataset\n",
        "                - word_idx_map: mapping of each word from vocab to its index in W\n",
        "                - label_index: mapping of each label (emotion or sentiment) to its assigned index, eg. label_index['neutral']=0\n",
        "        \"\"\"\n",
        "        x = pickle.load(\n",
        "            open(\"./features/data_{}.p\".format(self.MODE.lower()), \"rb\"))\n",
        "        revs, self.W, self.word_idx_map, self.vocab, _, label_index = x[\n",
        "            0], x[1], x[2], x[3], x[4], x[5]\n",
        "        self.num_classes = len(label_index)\n",
        "        print(\"Labels used for this classification: \", label_index)\n",
        "\n",
        "        # Preparing data\n",
        "        self.train_data, self.val_data, self.test_data = {}, {}, {}\n",
        "        for i in range(len(revs)):\n",
        "\n",
        "            utterance_id = revs[i]['dialog']+\"_\"+revs[i]['utterance']\n",
        "            sentence_word_indices = self.get_word_indices(revs[i]['text'])\n",
        "            label = label_index[revs[i]['y']]\n",
        "\n",
        "            if revs[i]['split'] == \"train\":\n",
        "                self.train_data[utterance_id] = (sentence_word_indices, label)\n",
        "            elif revs[i]['split'] == \"val\":\n",
        "                self.val_data[utterance_id] = (sentence_word_indices, label)\n",
        "            elif revs[i]['split'] == \"test\":\n",
        "                self.test_data[utterance_id] = (sentence_word_indices, label)\n",
        "\n",
        "        # Creating dialogue:[utterance_1, utterance_2, ...] ids\n",
        "        self.train_dialogue_ids = self.get_dialogue_ids(self.train_data.keys())\n",
        "        self.val_dialogue_ids = self.get_dialogue_ids(self.val_data.keys())\n",
        "        self.test_dialogue_ids = self.get_dialogue_ids(self.test_data.keys())\n",
        "\n",
        "        # Max utternance in a dialog in the dataset\n",
        "        self.max_utts = self.get_max_utts(\n",
        "            self.train_dialogue_ids, self.val_dialogue_ids, self.test_dialogue_ids)\n",
        "\n",
        "    def get_word_indices(self, data_x):\n",
        "        length = len(data_x.split())\n",
        "        return np.array([self.word_idx_map[word] for word in data_x.split()] + [0]*(self.max_l-length))[:self.max_l]\n",
        "\n",
        "    def get_dialogue_ids(self, keys):\n",
        "        ids = defaultdict(list)\n",
        "        for key in keys:\n",
        "            ids[key.split(\"_\")[0]].append(int(key.split(\"_\")[1]))\n",
        "        for ID, utts in ids.items():\n",
        "            ids[ID] = [str(utt) for utt in sorted(utts)]\n",
        "        return ids\n",
        "\n",
        "    def get_max_utts(self, train_ids, val_ids, test_ids):\n",
        "        max_utts_train = max([len(train_ids[vid]) for vid in train_ids.keys()])\n",
        "        max_utts_val = max([len(val_ids[vid]) for vid in val_ids.keys()])\n",
        "        max_utts_test = max([len(test_ids[vid]) for vid in test_ids.keys()])\n",
        "        return np.max([max_utts_train, max_utts_val, max_utts_test])\n",
        "\n",
        "    def get_one_hot(self, label):\n",
        "        label_arr = [0]*self.num_classes\n",
        "        label_arr[label] = 1\n",
        "        return label_arr[:]\n",
        "\n",
        "    def get_dialogue_audio_embs(self):\n",
        "        key = list(self.train_audio_emb.keys())[0]\n",
        "        pad = [0]*len(self.train_audio_emb[key])\n",
        "\n",
        "        def get_emb(dialogue_id, audio_emb):\n",
        "            dialogue_audio = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_audio = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        local_audio.append(audio_emb[vid+\"_\"+str(utt)][:])\n",
        "                    except:\n",
        "                        print(vid+\"_\"+str(utt))\n",
        "                        local_audio.append(pad[:])\n",
        "                for _ in range(self.max_utts-len(local_audio)):\n",
        "                    local_audio.append(pad[:])\n",
        "                dialogue_audio.append(local_audio[:self.max_utts])\n",
        "            return np.array(dialogue_audio)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(\n",
        "            self.train_dialogue_ids, self.train_audio_emb)\n",
        "        self.val_dialogue_features = get_emb(\n",
        "            self.val_dialogue_ids, self.val_audio_emb)\n",
        "        self.test_dialogue_features = get_emb(\n",
        "            self.test_dialogue_ids, self.test_audio_emb)\n",
        "\n",
        "    def get_dialogue_text_embs(self):\n",
        "        key = list(self.train_data.keys())[0]\n",
        "        pad = [0]*len(self.train_data[key][0])\n",
        "\n",
        "        def get_emb(dialogue_id, local_data):\n",
        "            dialogue_text = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_text = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    local_text.append(local_data[vid+\"_\"+str(utt)][0][:])\n",
        "                for _ in range(self.max_utts-len(local_text)):\n",
        "                    local_text.append(pad[:])\n",
        "                dialogue_text.append(local_text[:self.max_utts])\n",
        "            return np.array(dialogue_text)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(\n",
        "            self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_features = get_emb(\n",
        "            self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_features = get_emb(\n",
        "            self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels(self):\n",
        "\n",
        "        def get_labels(ids, data):\n",
        "            dialogue_label = []\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels = []\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(\n",
        "                        data[vid+\"_\"+str(utt)][1]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1))  # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label = get_labels(\n",
        "            self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_label = get_labels(\n",
        "            self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_label = get_labels(\n",
        "            self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_lengths(self):\n",
        "\n",
        "        self.train_dialogue_length, self.val_dialogue_length, self.test_dialogue_length = [], [], []\n",
        "        for vid, utts in self.train_dialogue_ids.items():\n",
        "            self.train_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.val_dialogue_ids.items():\n",
        "            self.val_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.test_dialogue_ids.items():\n",
        "            self.test_dialogue_length.append(len(utts))\n",
        "\n",
        "    def get_masks(self):\n",
        "\n",
        "        self.train_mask = np.zeros(\n",
        "            (len(self.train_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.train_dialogue_length)):\n",
        "            self.train_mask[i, :self.train_dialogue_length[i]] = 1.0\n",
        "        self.val_mask = np.zeros(\n",
        "            (len(self.val_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.val_dialogue_length)):\n",
        "            self.val_mask[i, :self.val_dialogue_length[i]] = 1.0\n",
        "        self.test_mask = np.zeros(\n",
        "            (len(self.test_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.test_dialogue_length)):\n",
        "            self.test_mask[i, :self.test_dialogue_length[i]] = 1.0\n",
        "\n",
        "    def load_audio_data(self, ):\n",
        "\n",
        "        AUDIO_PATH = \"./features/audio_embeddings_feature_selection_{}.pkl\".format(\n",
        "            self.MODE.lower())\n",
        "        self.train_audio_emb, self.val_audio_emb, self.test_audio_emb = pickle.load(\n",
        "            open(AUDIO_PATH, \"rb\"))\n",
        "\n",
        "        self.get_dialogue_audio_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_text_data(self, ):\n",
        "\n",
        "        self.get_dialogue_text_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_bimodal_data(self,):\n",
        "\n",
        "        TEXT_UNIMODAL = \"./features/text_{}.pkl\".format(self.MODE.lower())\n",
        "        AUDIO_UNIMODAL = \"./features/audio_{}.pkl\".format(\n",
        "            self.MODE.lower())\n",
        "\n",
        "        # Load features\n",
        "        train_text_x, val_text_x, test_text_x = pickle.load(\n",
        "            open(TEXT_UNIMODAL, \"rb\"), encoding='latin1')\n",
        "        train_audio_x, val_audio_x, test_audio_x = pickle.load(\n",
        "            open(AUDIO_UNIMODAL, \"rb\"), encoding='latin1')\n",
        "\n",
        "        def concatenate_fusion(ID, text, audio):\n",
        "            bimodal = []\n",
        "            for vid, utts in ID.items():\n",
        "                bimodal.append(np.concatenate((text[vid], audio[vid]), axis=1))\n",
        "            return np.array(bimodal)\n",
        "\n",
        "        self.train_dialogue_features = concatenate_fusion(\n",
        "            self.train_dialogue_ids, train_text_x, train_audio_x)\n",
        "        self.val_dialogue_features = concatenate_fusion(\n",
        "            self.val_dialogue_ids, val_text_x, val_audio_x)\n",
        "        self.test_dialogue_features = concatenate_fusion(\n",
        "            self.test_dialogue_ids, test_text_x, test_audio_x)\n",
        "\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib_6wFXmWzgi"
      },
      "source": [
        "# Define model classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqeBdIwDW17n"
      },
      "source": [
        "class bc_LSTM:\n",
        "\n",
        "    def __init__(self, classification_mode, modality):\n",
        "        self.classification_mode = classification_mode\n",
        "        self.modality = modality\n",
        "\n",
        "        self.PATH = \"./models/{}_weights_{}.hdf5\".format(\n",
        "            self.modality, self.classification_mode)\n",
        "        self.OUTPUT_PATH = \"./features/{}_{}.pkl\".format(\n",
        "            self.modality, self.classification_mode)\n",
        "        print(\"Model initiated for {} classification\".format(\n",
        "            self.classification_mode))\n",
        "\n",
        "    def load_data(self,):\n",
        "\n",
        "        print('Loading data')\n",
        "        self.data = Dataloader(mode=self.classification_mode)\n",
        "\n",
        "        if self.modality == \"text\":\n",
        "            self.data.load_text_data()\n",
        "        elif self.modality == \"audio\":\n",
        "            self.data.load_audio_data()\n",
        "        elif self.modality == \"bimodal\":\n",
        "            self.data.load_bimodal_data()\n",
        "        else:\n",
        "            exit()\n",
        "\n",
        "        self.train_x = self.data.train_dialogue_features\n",
        "        self.val_x = self.data.val_dialogue_features\n",
        "        self.test_x = self.data.test_dialogue_features\n",
        "\n",
        "        self.train_y = self.data.train_dialogue_label\n",
        "        self.val_y = self.data.val_dialogue_label\n",
        "        self.test_y = self.data.test_dialogue_label\n",
        "\n",
        "        self.train_mask = self.data.train_mask\n",
        "        self.val_mask = self.data.val_mask\n",
        "        self.test_mask = self.data.test_mask\n",
        "\n",
        "        self.train_id = self.data.train_dialogue_ids.keys()\n",
        "        self.val_id = self.data.val_dialogue_ids.keys()\n",
        "        self.test_id = self.data.test_dialogue_ids.keys()\n",
        "\n",
        "        self.sequence_length = self.train_x.shape[1]\n",
        "\n",
        "        self.classes = self.train_y.shape[2]\n",
        "\n",
        "    def calc_test_result(self, pred_label, test_label, test_mask):\n",
        "\n",
        "        true_label = []\n",
        "        predicted_label = []\n",
        "\n",
        "        for i in range(pred_label.shape[0]):\n",
        "            for j in range(pred_label.shape[1]):\n",
        "                if test_mask[i, j] == 1:\n",
        "                    true_label.append(np.argmax(test_label[i, j]))\n",
        "                    predicted_label.append(np.argmax(pred_label[i, j]))\n",
        "        print(\"Confusion Matrix :\")\n",
        "        print(confusion_matrix(true_label, predicted_label))\n",
        "        print(\"Classification Report :\")\n",
        "        print(classification_report(true_label, predicted_label, digits=4))\n",
        "        print('Weighted FScore: \\n ', precision_recall_fscore_support(\n",
        "            true_label, predicted_label, average='weighted'))\n",
        "\n",
        "    def get_audio_model(self):\n",
        "\n",
        "        # Modality specific hyperparameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 50\n",
        "\n",
        "        # Modality specific parameters\n",
        "        self.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "        print(\"Creating Model...\")\n",
        "\n",
        "        inputs = Input(shape=(self.sequence_length,\n",
        "                              self.embedding_dim), dtype='float32')\n",
        "        masked = Masking(mask_value=0)(inputs)\n",
        "        lstm = Bidirectional(\n",
        "            LSTM(300, activation='tanh', return_sequences=True, dropout=0.4))(masked)\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            300, activation='tanh', return_sequences=True, dropout=0.4), name=\"utter\")(lstm)\n",
        "        output = TimeDistributed(\n",
        "            Dense(self.classes, activation='softmax'))(lstm)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        return model\n",
        "\n",
        "    def get_text_model(self):\n",
        "\n",
        "        # Modality specific hyperparameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 50\n",
        "\n",
        "        # Modality specific parameters\n",
        "        self.embedding_dim = self.data.W.shape[1]\n",
        "\n",
        "        # For text model\n",
        "        self.vocabulary_size = self.data.W.shape[0]\n",
        "        self.filter_sizes = [3, 4, 5]\n",
        "        self.num_filters = 512\n",
        "\n",
        "        print(\"Creating Model...\")\n",
        "\n",
        "        sentence_length = self.train_x.shape[2]\n",
        "\n",
        "        # Initializing sentence representation layers\n",
        "        embedding = Embedding(input_dim=self.vocabulary_size, output_dim=self.embedding_dim, weights=[\n",
        "                              self.data.W], input_length=sentence_length, trainable=False)\n",
        "        conv_0 = Conv2D(self.num_filters, kernel_size=(\n",
        "            self.filter_sizes[0], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "        conv_1 = Conv2D(self.num_filters, kernel_size=(\n",
        "            self.filter_sizes[1], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "        conv_2 = Conv2D(self.num_filters, kernel_size=(\n",
        "            self.filter_sizes[2], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "        maxpool_0 = MaxPool2D(pool_size=(\n",
        "            sentence_length - self.filter_sizes[0] + 1, 1), strides=(1, 1), padding='valid')\n",
        "        maxpool_1 = MaxPool2D(pool_size=(\n",
        "            sentence_length - self.filter_sizes[1] + 1, 1), strides=(1, 1), padding='valid')\n",
        "        maxpool_2 = MaxPool2D(pool_size=(\n",
        "            sentence_length - self.filter_sizes[2] + 1, 1), strides=(1, 1), padding='valid')\n",
        "        dense_func = Dense(100, activation='tanh', name=\"dense\")\n",
        "        dense_final = Dense(units=self.classes, activation='softmax')\n",
        "        reshape_func = Reshape((sentence_length, self.embedding_dim, 1))\n",
        "\n",
        "        def slicer(x, index):\n",
        "            return x[:, K.constant(index, dtype='int32'), :]\n",
        "\n",
        "        def slicer_output_shape(input_shape):\n",
        "            shape = list(input_shape)\n",
        "            assert len(shape) == 3  # batch, seq_len, sent_len\n",
        "            new_shape = (shape[0], shape[2])\n",
        "            return new_shape\n",
        "\n",
        "        def reshaper(x):\n",
        "            return K.expand_dims(x, axis=3)\n",
        "\n",
        "        def flattener(x):\n",
        "            x = K.reshape(x, [-1, x.shape[1]*x.shape[3]])\n",
        "            return x\n",
        "\n",
        "        def flattener_output_shape(input_shape):\n",
        "            shape = list(input_shape)\n",
        "            new_shape = (shape[0], 3*shape[3])\n",
        "            return new_shape\n",
        "\n",
        "        inputs = Input(shape=(self.sequence_length,\n",
        "                              sentence_length), dtype='int32')\n",
        "        cnn_output = []\n",
        "        for ind in range(self.sequence_length):\n",
        "\n",
        "            local_input = Lambda(slicer, output_shape=slicer_output_shape, arguments={\n",
        "                                 \"index\": ind})(inputs)  # Batch, word_indices\n",
        "\n",
        "            # cnn-sent\n",
        "            emb_output = embedding(local_input)\n",
        "            reshape = Lambda(reshaper)(emb_output)\n",
        "            concatenated_tensor = Concatenate(axis=1)([maxpool_0(\n",
        "                conv_0(reshape)), maxpool_1(conv_1(reshape)), maxpool_2(conv_2(reshape))])\n",
        "            flatten = Lambda(flattener, output_shape=flattener_output_shape,)(\n",
        "                concatenated_tensor)\n",
        "            dense_output = dense_func(flatten)\n",
        "            dropout = Dropout(0.5)(dense_output)\n",
        "            cnn_output.append(dropout)\n",
        "\n",
        "        def stack(x):\n",
        "            return K.stack(x, axis=1)\n",
        "        cnn_outputs = Lambda(stack)(cnn_output)\n",
        "\n",
        "        masked = Masking(mask_value=0)(cnn_outputs)\n",
        "        lstm = Bidirectional(\n",
        "            LSTM(300, activation='relu', return_sequences=True, dropout=0.3))(masked)\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            300, activation='relu', return_sequences=True, dropout=0.3), name=\"utter\")(lstm)\n",
        "        output = TimeDistributed(\n",
        "            Dense(self.classes, activation='softmax'))(lstm)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        return model\n",
        "\n",
        "    def get_bimodal_model(self):\n",
        "\n",
        "        # Modality specific hyperparameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 10\n",
        "\n",
        "        # Modality specific parameters\n",
        "        self.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "        print(\"Creating Model...\")\n",
        "\n",
        "        inputs = Input(shape=(self.sequence_length,\n",
        "                              self.embedding_dim), dtype='float32')\n",
        "        masked = Masking(mask_value=0)(inputs)\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            300, activation='tanh', return_sequences=True, dropout=0.4), name=\"utter\")(masked)\n",
        "        output = TimeDistributed(\n",
        "            Dense(self.classes, activation='softmax'))(lstm)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        return model\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            self.PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "        if self.modality == \"audio\":\n",
        "            model = self.get_audio_model()\n",
        "            model.compile(\n",
        "                optimizer='adadelta', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "        elif self.modality == \"text\":\n",
        "            model = self.get_text_model()\n",
        "            model.compile(\n",
        "                optimizer='adadelta', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "        elif self.modality == \"bimodal\":\n",
        "            model = self.get_bimodal_model()\n",
        "            model.compile(\n",
        "                optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "        model.fit(self.train_x, self.train_y,\n",
        "                  epochs=self.epochs,\n",
        "                  batch_size=self.batch_size,\n",
        "                  sample_weight=self.train_mask,\n",
        "                  shuffle=True,\n",
        "                  callbacks=[early_stopping, checkpoint],\n",
        "                  validation_data=(self.val_x, self.val_y, self.val_mask))\n",
        "\n",
        "        self.test_model()\n",
        "\n",
        "    def test_model(self):\n",
        "\n",
        "        model = load_model(self.PATH)\n",
        "        intermediate_layer_model = Model(\n",
        "            inputs=model.input, outputs=model.get_layer(\"utter\").output)\n",
        "\n",
        "        intermediate_output_train = intermediate_layer_model.predict(\n",
        "            self.train_x)\n",
        "        intermediate_output_val = intermediate_layer_model.predict(self.val_x)\n",
        "        intermediate_output_test = intermediate_layer_model.predict(\n",
        "            self.test_x)\n",
        "\n",
        "        train_emb, val_emb, test_emb = {}, {}, {}\n",
        "        for idx, ID in enumerate(self.train_id):\n",
        "            train_emb[ID] = intermediate_output_train[idx]\n",
        "        for idx, ID in enumerate(self.val_id):\n",
        "            val_emb[ID] = intermediate_output_val[idx]\n",
        "        for idx, ID in enumerate(self.test_id):\n",
        "            test_emb[ID] = intermediate_output_test[idx]\n",
        "        pickle.dump([train_emb, val_emb, test_emb],\n",
        "                    open(self.OUTPUT_PATH, \"wb\"))\n",
        "\n",
        "        self.calc_test_result(model.predict(self.test_x),\n",
        "                              self.test_y, self.test_mask)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB8f0K_wRd5C"
      },
      "source": [
        "# Run either train or test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akEyXrKXQoHU",
        "outputId": "9aa4715e-7223-4a92-ff6f-1af521465be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "classification_mode = 'emotion' # Should be either 'emotion' or 'sentiment'\n",
        "modality = 'bimodal' # Should be one of 'text', 'audio', or 'bimodal'\n",
        "train_or_test = 'test' # Should be either 'train' or 'test'\n",
        "\n",
        "model = bc_LSTM(classification_mode, modality)\n",
        "model.load_data()\n",
        "\n",
        "if train_or_test == 'train':\n",
        "    model.train_model()\n",
        "else:\n",
        "    model.test_model()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initiated for emotion classification\n",
            "Loading data\n",
            "Labels used for this classification:  {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger': 6}\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Confusion Matrix :\n",
            "[[1036   46    0   29   82    0   63]\n",
            " [  73  125    0    2   37    0   44]\n",
            " [  26    2    0    1    7    0   14]\n",
            " [ 104   12    0   32   10    0   50]\n",
            " [ 107   25    0    2  204    0   64]\n",
            " [  30    9    0    5    2    0   22]\n",
            " [  84   28    0    8   47    0  178]]\n",
            "Classification Report :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7096    0.8248    0.7629      1256\n",
            "           1     0.5061    0.4448    0.4735       281\n",
            "           2     0.0000    0.0000    0.0000        50\n",
            "           3     0.4051    0.1538    0.2230       208\n",
            "           4     0.5244    0.5075    0.5158       402\n",
            "           5     0.0000    0.0000    0.0000        68\n",
            "           6     0.4092    0.5159    0.4564       345\n",
            "\n",
            "    accuracy                         0.6034      2610\n",
            "   macro avg     0.3649    0.3496    0.3474      2610\n",
            "weighted avg     0.5631    0.6034    0.5756      2610\n",
            "\n",
            "Weighted FScore: \n",
            "  (0.563100908972832, 0.603448275862069, 0.5756445751861287, None)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}