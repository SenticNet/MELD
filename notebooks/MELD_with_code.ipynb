{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MELD_with_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOln4zCUWz6ee6Y2khzBJ1U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tae898/MELD/blob/master/notebooks/MELD_with_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnIWWiRT8yhg"
      },
      "source": [
        "# Download the pre-computed features and models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqTYE5HU81Qc",
        "outputId": "8739f960-cb80-46e5-ce0a-2fb943739d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "# wget to download\n",
        "!wget http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Features.Models.tar.gz\n",
        "# untar\n",
        "!tar -zxvf MELD.Features.Models.tar.gz\n",
        "# remove the tar\n",
        "!rm -rf MELD.Features.Models.tar.gz\n",
        "\n",
        "# move the features and the models to the curret directory\n",
        "!mv MELD.Features.Models/features/ ./features\n",
        "!mv MELD.Features.Models/models/ ./models"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-11 10:38:31--  http://web.eecs.umich.edu/~mihalcea/downloads/MELD.Features.Models.tar.gz\n",
            "Resolving web.eecs.umich.edu (web.eecs.umich.edu)... 141.212.113.214\n",
            "Connecting to web.eecs.umich.edu (web.eecs.umich.edu)|141.212.113.214|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 922694594 (880M) [application/x-gzip]\n",
            "Saving to: ‘MELD.Features.Models.tar.gz.1’\n",
            "\n",
            "MELD.Features.Model 100%[===================>] 879.95M  54.8MB/s    in 17s     \n",
            "\n",
            "2020-10-11 10:38:48 (51.5 MB/s) - ‘MELD.Features.Models.tar.gz.1’ saved [922694594/922694594]\n",
            "\n",
            "MELD.Features.Models/\n",
            "MELD.Features.Models/models/\n",
            "MELD.Features.Models/models/audio_weights_emotion.hdf5\n",
            "MELD.Features.Models/models/text_weights_emotion.hdf5\n",
            "MELD.Features.Models/models/bimodal_weights_emotion.hdf5\n",
            "MELD.Features.Models/models/text_weights_sentiment.hdf5\n",
            "MELD.Features.Models/models/audio_weights_sentiment.hdf5\n",
            "MELD.Features.Models/models/bimodal_weights_sentiment.hdf5\n",
            "MELD.Features.Models/features/\n",
            "MELD.Features.Models/features/text_glove_CNN_sentiment.pkl\n",
            "MELD.Features.Models/features/text_glove_CNN_emotion.pkl\n",
            "MELD.Features.Models/features/text_emotion.pkl\n",
            "MELD.Features.Models/features/bimodal_sentiment.pkl\n",
            "MELD.Features.Models/features/text_glove_average_emotion.pkl\n",
            "MELD.Features.Models/features/audio_embeddings_feature_selection_sentiment.pkl\n",
            "MELD.Features.Models/features/data_sentiment.p\n",
            "MELD.Features.Models/features/data_emotion.p\n",
            "MELD.Features.Models/features/text_glove_average_sentiment.pkl\n",
            "MELD.Features.Models/features/audio_sentiment.pkl\n",
            "MELD.Features.Models/features/audio_embeddings_feature_selection_emotion.pkl\n",
            "MELD.Features.Models/features/text_sentiment.pkl\n",
            "MELD.Features.Models/features/audio_emotion.pkl\n",
            "MELD.Features.Models/README.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PJaFxr9QFRQ"
      },
      "source": [
        "# Import the necessary packages, define classes and functions, and set the values of the hyperparameters\n",
        "\n",
        "These are mostly copied from [here](https://github.com/declare-lab/MELD/tree/master/baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVhDLg-FQE58"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import sys\n",
        "from collections import Counter, defaultdict\n",
        "import argparse\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, Bidirectional\n",
        "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "import keras.backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "###################################################################################################################################\n",
        "\n",
        "# Hyperparams\n",
        "max_length = 50  # Maximum length of the sentence\n",
        "\n",
        "\n",
        "class Dataloader:\n",
        "\n",
        "    def __init__(self, mode=None):\n",
        "\n",
        "        try:\n",
        "            assert(mode is not None)\n",
        "        except AssertionError as e:\n",
        "            print(\"Set mode as 'Sentiment' or 'Emotion'\")\n",
        "            exit()\n",
        "\n",
        "        self.MODE = mode  # Sentiment or Emotion classification mode\n",
        "        self.max_l = max_length\n",
        "\n",
        "        \"\"\"\n",
        "            Loading the dataset: \n",
        "                - revs is a dictionary with keys/value: \n",
        "                    - text: original sentence\n",
        "                    - split: train/val/test :: denotes the which split the tuple belongs to\n",
        "                    - y: label of the sentence\n",
        "                    - dialog: ID of the dialog the utterance belongs to\n",
        "                    - utterance: utterance number of the dialog ID\n",
        "                    - num_words: number of words in the utterance\n",
        "                - W: glove embedding matrix\n",
        "                - vocab: the vocabulary of the dataset\n",
        "                - word_idx_map: mapping of each word from vocab to its index in W\n",
        "                - label_index: mapping of each label (emotion or sentiment) to its assigned index, eg. label_index['neutral']=0\n",
        "        \"\"\"\n",
        "        x = pickle.load(\n",
        "            open(\"./data/pickles/data_{}.p\".format(self.MODE.lower()), \"rb\"))\n",
        "        revs, self.W, self.word_idx_map, self.vocab, _, label_index = x[\n",
        "            0], x[1], x[2], x[3], x[4], x[5]\n",
        "        self.num_classes = len(label_index)\n",
        "        print(\"Labels used for this classification: \", label_index)\n",
        "\n",
        "        # Preparing data\n",
        "        self.train_data, self.val_data, self.test_data = {}, {}, {}\n",
        "        for i in range(len(revs)):\n",
        "\n",
        "            utterance_id = revs[i]['dialog']+\"_\"+revs[i]['utterance']\n",
        "            sentence_word_indices = self.get_word_indices(revs[i]['text'])\n",
        "            label = label_index[revs[i]['y']]\n",
        "\n",
        "            if revs[i]['split'] == \"train\":\n",
        "                self.train_data[utterance_id] = (sentence_word_indices, label)\n",
        "            elif revs[i]['split'] == \"val\":\n",
        "                self.val_data[utterance_id] = (sentence_word_indices, label)\n",
        "            elif revs[i]['split'] == \"test\":\n",
        "                self.test_data[utterance_id] = (sentence_word_indices, label)\n",
        "\n",
        "        # Creating dialogue:[utterance_1, utterance_2, ...] ids\n",
        "        self.train_dialogue_ids = self.get_dialogue_ids(self.train_data.keys())\n",
        "        self.val_dialogue_ids = self.get_dialogue_ids(self.val_data.keys())\n",
        "        self.test_dialogue_ids = self.get_dialogue_ids(self.test_data.keys())\n",
        "\n",
        "        # Max utternance in a dialog in the dataset\n",
        "        self.max_utts = self.get_max_utts(\n",
        "            self.train_dialogue_ids, self.val_dialogue_ids, self.test_dialogue_ids)\n",
        "\n",
        "    def get_word_indices(self, data_x):\n",
        "        length = len(data_x.split())\n",
        "        return np.array([self.word_idx_map[word] for word in data_x.split()] + [0]*(self.max_l-length))[:self.max_l]\n",
        "\n",
        "    def get_dialogue_ids(self, keys):\n",
        "        ids = defaultdict(list)\n",
        "        for key in keys:\n",
        "            ids[key.split(\"_\")[0]].append(int(key.split(\"_\")[1]))\n",
        "        for ID, utts in ids.items():\n",
        "            ids[ID] = [str(utt) for utt in sorted(utts)]\n",
        "        return ids\n",
        "\n",
        "    def get_max_utts(self, train_ids, val_ids, test_ids):\n",
        "        max_utts_train = max([len(train_ids[vid]) for vid in train_ids.keys()])\n",
        "        max_utts_val = max([len(val_ids[vid]) for vid in val_ids.keys()])\n",
        "        max_utts_test = max([len(test_ids[vid]) for vid in test_ids.keys()])\n",
        "        return np.max([max_utts_train, max_utts_val, max_utts_test])\n",
        "\n",
        "    def get_one_hot(self, label):\n",
        "        label_arr = [0]*self.num_classes\n",
        "        label_arr[label] = 1\n",
        "        return label_arr[:]\n",
        "\n",
        "    def get_dialogue_audio_embs(self):\n",
        "        key = list(self.train_audio_emb.keys())[0]\n",
        "        pad = [0]*len(self.train_audio_emb[key])\n",
        "\n",
        "        def get_emb(dialogue_id, audio_emb):\n",
        "            dialogue_audio = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_audio = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    try:\n",
        "                        local_audio.append(audio_emb[vid+\"_\"+str(utt)][:])\n",
        "                    except:\n",
        "                        print(vid+\"_\"+str(utt))\n",
        "                        local_audio.append(pad[:])\n",
        "                for _ in range(self.max_utts-len(local_audio)):\n",
        "                    local_audio.append(pad[:])\n",
        "                dialogue_audio.append(local_audio[:self.max_utts])\n",
        "            return np.array(dialogue_audio)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(\n",
        "            self.train_dialogue_ids, self.train_audio_emb)\n",
        "        self.val_dialogue_features = get_emb(\n",
        "            self.val_dialogue_ids, self.val_audio_emb)\n",
        "        self.test_dialogue_features = get_emb(\n",
        "            self.test_dialogue_ids, self.test_audio_emb)\n",
        "\n",
        "    def get_dialogue_text_embs(self):\n",
        "        key = list(self.train_data.keys())[0]\n",
        "        pad = [0]*len(self.train_data[key][0])\n",
        "\n",
        "        def get_emb(dialogue_id, local_data):\n",
        "            dialogue_text = []\n",
        "            for vid in dialogue_id.keys():\n",
        "                local_text = []\n",
        "                for utt in dialogue_id[vid]:\n",
        "                    local_text.append(local_data[vid+\"_\"+str(utt)][0][:])\n",
        "                for _ in range(self.max_utts-len(local_text)):\n",
        "                    local_text.append(pad[:])\n",
        "                dialogue_text.append(local_text[:self.max_utts])\n",
        "            return np.array(dialogue_text)\n",
        "\n",
        "        self.train_dialogue_features = get_emb(\n",
        "            self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_features = get_emb(\n",
        "            self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_features = get_emb(\n",
        "            self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_labels(self):\n",
        "\n",
        "        def get_labels(ids, data):\n",
        "            dialogue_label = []\n",
        "\n",
        "            for vid, utts in ids.items():\n",
        "                local_labels = []\n",
        "                for utt in utts:\n",
        "                    local_labels.append(self.get_one_hot(\n",
        "                        data[vid+\"_\"+str(utt)][1]))\n",
        "                for _ in range(self.max_utts-len(local_labels)):\n",
        "                    local_labels.append(self.get_one_hot(1))  # Dummy label\n",
        "                dialogue_label.append(local_labels[:self.max_utts])\n",
        "            return np.array(dialogue_label)\n",
        "\n",
        "        self.train_dialogue_label = get_labels(\n",
        "            self.train_dialogue_ids, self.train_data)\n",
        "        self.val_dialogue_label = get_labels(\n",
        "            self.val_dialogue_ids, self.val_data)\n",
        "        self.test_dialogue_label = get_labels(\n",
        "            self.test_dialogue_ids, self.test_data)\n",
        "\n",
        "    def get_dialogue_lengths(self):\n",
        "\n",
        "        self.train_dialogue_length, self.val_dialogue_length, self.test_dialogue_length = [], [], []\n",
        "        for vid, utts in self.train_dialogue_ids.items():\n",
        "            self.train_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.val_dialogue_ids.items():\n",
        "            self.val_dialogue_length.append(len(utts))\n",
        "        for vid, utts in self.test_dialogue_ids.items():\n",
        "            self.test_dialogue_length.append(len(utts))\n",
        "\n",
        "    def get_masks(self):\n",
        "\n",
        "        self.train_mask = np.zeros(\n",
        "            (len(self.train_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.train_dialogue_length)):\n",
        "            self.train_mask[i, :self.train_dialogue_length[i]] = 1.0\n",
        "        self.val_mask = np.zeros(\n",
        "            (len(self.val_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.val_dialogue_length)):\n",
        "            self.val_mask[i, :self.val_dialogue_length[i]] = 1.0\n",
        "        self.test_mask = np.zeros(\n",
        "            (len(self.test_dialogue_length), self.max_utts), dtype='float')\n",
        "        for i in range(len(self.test_dialogue_length)):\n",
        "            self.test_mask[i, :self.test_dialogue_length[i]] = 1.0\n",
        "\n",
        "    def load_audio_data(self, ):\n",
        "\n",
        "        AUDIO_PATH = \"./data/pickles/audio_embeddings_feature_selection_{}.pkl\".format(\n",
        "            self.MODE.lower())\n",
        "        self.train_audio_emb, self.val_audio_emb, self.test_audio_emb = pickle.load(\n",
        "            open(AUDIO_PATH, \"rb\"))\n",
        "\n",
        "        self.get_dialogue_audio_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_text_data(self, ):\n",
        "\n",
        "        self.get_dialogue_text_embs()\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "    def load_bimodal_data(self,):\n",
        "\n",
        "        TEXT_UNIMODAL = \"./data/pickles/text_{}.pkl\".format(self.MODE.lower())\n",
        "        AUDIO_UNIMODAL = \"./data/pickles/audio_{}.pkl\".format(\n",
        "            self.MODE.lower())\n",
        "\n",
        "        # Load features\n",
        "        train_text_x, val_text_x, test_text_x = pickle.load(\n",
        "            open(TEXT_UNIMODAL, \"rb\"), encoding='latin1')\n",
        "        train_audio_x, val_audio_x, test_audio_x = pickle.load(\n",
        "            open(AUDIO_UNIMODAL, \"rb\"), encoding='latin1')\n",
        "\n",
        "        def concatenate_fusion(ID, text, audio):\n",
        "            bimodal = []\n",
        "            for vid, utts in ID.items():\n",
        "                bimodal.append(np.concatenate((text[vid], audio[vid]), axis=1))\n",
        "            return np.array(bimodal)\n",
        "\n",
        "        self.train_dialogue_features = concatenate_fusion(\n",
        "            self.train_dialogue_ids, train_text_x, train_audio_x)\n",
        "        self.val_dialogue_features = concatenate_fusion(\n",
        "            self.val_dialogue_ids, val_text_x, val_audio_x)\n",
        "        self.test_dialogue_features = concatenate_fusion(\n",
        "            self.test_dialogue_ids, test_text_x, test_audio_x)\n",
        "\n",
        "        self.get_dialogue_lengths()\n",
        "        self.get_dialogue_labels()\n",
        "        self.get_masks()\n",
        "\n",
        "\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "\n",
        "class bc_LSTM:\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.classification_mode = args.classify\n",
        "        self.modality = args.modality\n",
        "        self.PATH = \"./data/models/{}_weights_{}.hdf5\".format(\n",
        "            args.modality, self.classification_mode.lower())\n",
        "        self.OUTPUT_PATH = \"./data/pickles/{}_{}.pkl\".format(\n",
        "            args.modality, self.classification_mode.lower())\n",
        "        print(\"Model initiated for {} classification\".format(\n",
        "            self.classification_mode))\n",
        "\n",
        "    def load_data(self,):\n",
        "\n",
        "        print('Loading data')\n",
        "        self.data = Dataloader(mode=self.classification_mode)\n",
        "\n",
        "        if self.modality == \"text\":\n",
        "            self.data.load_text_data()\n",
        "        elif self.modality == \"audio\":\n",
        "            self.data.load_audio_data()\n",
        "        elif self.modality == \"bimodal\":\n",
        "            self.data.load_bimodal_data()\n",
        "        else:\n",
        "            exit()\n",
        "\n",
        "        self.train_x = self.data.train_dialogue_features\n",
        "        self.val_x = self.data.val_dialogue_features\n",
        "        self.test_x = self.data.test_dialogue_features\n",
        "\n",
        "        self.train_y = self.data.train_dialogue_label\n",
        "        self.val_y = self.data.val_dialogue_label\n",
        "        self.test_y = self.data.test_dialogue_label\n",
        "\n",
        "        self.train_mask = self.data.train_mask\n",
        "        self.val_mask = self.data.val_mask\n",
        "        self.test_mask = self.data.test_mask\n",
        "\n",
        "        self.train_id = self.data.train_dialogue_ids.keys()\n",
        "        self.val_id = self.data.val_dialogue_ids.keys()\n",
        "        self.test_id = self.data.test_dialogue_ids.keys()\n",
        "\n",
        "        self.sequence_length = self.train_x.shape[1]\n",
        "\n",
        "        self.classes = self.train_y.shape[2]\n",
        "\n",
        "    def calc_test_result(self, pred_label, test_label, test_mask):\n",
        "\n",
        "        true_label = []\n",
        "        predicted_label = []\n",
        "\n",
        "        for i in range(pred_label.shape[0]):\n",
        "            for j in range(pred_label.shape[1]):\n",
        "                if test_mask[i, j] == 1:\n",
        "                    true_label.append(np.argmax(test_label[i, j]))\n",
        "                    predicted_label.append(np.argmax(pred_label[i, j]))\n",
        "        print(\"Confusion Matrix :\")\n",
        "        print(confusion_matrix(true_label, predicted_label))\n",
        "        print(\"Classification Report :\")\n",
        "        print(classification_report(true_label, predicted_label, digits=4))\n",
        "        print('Weighted FScore: \\n ', precision_recall_fscore_support(\n",
        "            true_label, predicted_label, average='weighted'))\n",
        "\n",
        "    def get_audio_model(self):\n",
        "\n",
        "        # Modality specific hyperparameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 50\n",
        "\n",
        "        # Modality specific parameters\n",
        "        self.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "        print(\"Creating Model...\")\n",
        "\n",
        "        inputs = Input(shape=(self.sequence_length,\n",
        "                              self.embedding_dim), dtype='float32')\n",
        "        masked = Masking(mask_value=0)(inputs)\n",
        "        lstm = Bidirectional(\n",
        "            LSTM(300, activation='tanh', return_sequences=True, dropout=0.4))(masked)\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            300, activation='tanh', return_sequences=True, dropout=0.4), name=\"utter\")(lstm)\n",
        "        output = TimeDistributed(\n",
        "            Dense(self.classes, activation='softmax'))(lstm)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        return model\n",
        "\n",
        "    def get_text_model(self):\n",
        "\n",
        "        # Modality specific hyperparameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 50\n",
        "\n",
        "        # Modality specific parameters\n",
        "        self.embedding_dim = self.data.W.shape[1]\n",
        "\n",
        "        # For text model\n",
        "        self.vocabulary_size = self.data.W.shape[0]\n",
        "        self.filter_sizes = [3, 4, 5]\n",
        "        self.num_filters = 512\n",
        "\n",
        "        print(\"Creating Model...\")\n",
        "\n",
        "        sentence_length = self.train_x.shape[2]\n",
        "\n",
        "        # Initializing sentence representation layers\n",
        "        embedding = Embedding(input_dim=self.vocabulary_size, output_dim=self.embedding_dim, weights=[\n",
        "                              self.data.W], input_length=sentence_length, trainable=False)\n",
        "        conv_0 = Conv2D(self.num_filters, kernel_size=(\n",
        "            self.filter_sizes[0], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "        conv_1 = Conv2D(self.num_filters, kernel_size=(\n",
        "            self.filter_sizes[1], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "        conv_2 = Conv2D(self.num_filters, kernel_size=(\n",
        "            self.filter_sizes[2], self.embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')\n",
        "        maxpool_0 = MaxPool2D(pool_size=(\n",
        "            sentence_length - self.filter_sizes[0] + 1, 1), strides=(1, 1), padding='valid')\n",
        "        maxpool_1 = MaxPool2D(pool_size=(\n",
        "            sentence_length - self.filter_sizes[1] + 1, 1), strides=(1, 1), padding='valid')\n",
        "        maxpool_2 = MaxPool2D(pool_size=(\n",
        "            sentence_length - self.filter_sizes[2] + 1, 1), strides=(1, 1), padding='valid')\n",
        "        dense_func = Dense(100, activation='tanh', name=\"dense\")\n",
        "        dense_final = Dense(units=self.classes, activation='softmax')\n",
        "        reshape_func = Reshape((sentence_length, self.embedding_dim, 1))\n",
        "\n",
        "        def slicer(x, index):\n",
        "            return x[:, K.constant(index, dtype='int32'), :]\n",
        "\n",
        "        def slicer_output_shape(input_shape):\n",
        "            shape = list(input_shape)\n",
        "            assert len(shape) == 3  # batch, seq_len, sent_len\n",
        "            new_shape = (shape[0], shape[2])\n",
        "            return new_shape\n",
        "\n",
        "        def reshaper(x):\n",
        "            return K.expand_dims(x, axis=3)\n",
        "\n",
        "        def flattener(x):\n",
        "            x = K.reshape(x, [-1, x.shape[1]*x.shape[3]])\n",
        "            return x\n",
        "\n",
        "        def flattener_output_shape(input_shape):\n",
        "            shape = list(input_shape)\n",
        "            new_shape = (shape[0], 3*shape[3])\n",
        "            return new_shape\n",
        "\n",
        "        inputs = Input(shape=(self.sequence_length,\n",
        "                              sentence_length), dtype='int32')\n",
        "        cnn_output = []\n",
        "        for ind in range(self.sequence_length):\n",
        "\n",
        "            local_input = Lambda(slicer, output_shape=slicer_output_shape, arguments={\n",
        "                                 \"index\": ind})(inputs)  # Batch, word_indices\n",
        "\n",
        "            # cnn-sent\n",
        "            emb_output = embedding(local_input)\n",
        "            reshape = Lambda(reshaper)(emb_output)\n",
        "            concatenated_tensor = Concatenate(axis=1)([maxpool_0(\n",
        "                conv_0(reshape)), maxpool_1(conv_1(reshape)), maxpool_2(conv_2(reshape))])\n",
        "            flatten = Lambda(flattener, output_shape=flattener_output_shape,)(\n",
        "                concatenated_tensor)\n",
        "            dense_output = dense_func(flatten)\n",
        "            dropout = Dropout(0.5)(dense_output)\n",
        "            cnn_output.append(dropout)\n",
        "\n",
        "        def stack(x):\n",
        "            return K.stack(x, axis=1)\n",
        "        cnn_outputs = Lambda(stack)(cnn_output)\n",
        "\n",
        "        masked = Masking(mask_value=0)(cnn_outputs)\n",
        "        lstm = Bidirectional(\n",
        "            LSTM(300, activation='relu', return_sequences=True, dropout=0.3))(masked)\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            300, activation='relu', return_sequences=True, dropout=0.3), name=\"utter\")(lstm)\n",
        "        output = TimeDistributed(\n",
        "            Dense(self.classes, activation='softmax'))(lstm)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        return model\n",
        "\n",
        "    def get_bimodal_model(self):\n",
        "\n",
        "        # Modality specific hyperparameters\n",
        "        self.epochs = 100\n",
        "        self.batch_size = 10\n",
        "\n",
        "        # Modality specific parameters\n",
        "        self.embedding_dim = self.train_x.shape[2]\n",
        "\n",
        "        print(\"Creating Model...\")\n",
        "\n",
        "        inputs = Input(shape=(self.sequence_length,\n",
        "                              self.embedding_dim), dtype='float32')\n",
        "        masked = Masking(mask_value=0)(inputs)\n",
        "        lstm = Bidirectional(LSTM(\n",
        "            300, activation='tanh', return_sequences=True, dropout=0.4), name=\"utter\")(masked)\n",
        "        output = TimeDistributed(\n",
        "            Dense(self.classes, activation='softmax'))(lstm)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        return model\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            self.PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "        if self.modality == \"audio\":\n",
        "            model = self.get_audio_model()\n",
        "            model.compile(\n",
        "                optimizer='adadelta', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "        elif self.modality == \"text\":\n",
        "            model = self.get_text_model()\n",
        "            model.compile(\n",
        "                optimizer='adadelta', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "        elif self.modality == \"bimodal\":\n",
        "            model = self.get_bimodal_model()\n",
        "            model.compile(\n",
        "                optimizer='adam', loss='categorical_crossentropy', sample_weight_mode='temporal')\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "        model.fit(self.train_x, self.train_y,\n",
        "                  epochs=self.epochs,\n",
        "                  batch_size=self.batch_size,\n",
        "                  sample_weight=self.train_mask,\n",
        "                  shuffle=True,\n",
        "                  callbacks=[early_stopping, checkpoint],\n",
        "                  validation_data=(self.val_x, self.val_y, self.val_mask))\n",
        "\n",
        "        self.test_model()\n",
        "\n",
        "    def test_model(self):\n",
        "\n",
        "        model = load_model(self.PATH)\n",
        "        intermediate_layer_model = Model(\n",
        "            input=model.input, output=model.get_layer(\"utter\").output)\n",
        "\n",
        "        intermediate_output_train = intermediate_layer_model.predict(\n",
        "            self.train_x)\n",
        "        intermediate_output_val = intermediate_layer_model.predict(self.val_x)\n",
        "        intermediate_output_test = intermediate_layer_model.predict(\n",
        "            self.test_x)\n",
        "\n",
        "        train_emb, val_emb, test_emb = {}, {}, {}\n",
        "        for idx, ID in enumerate(self.train_id):\n",
        "            train_emb[ID] = intermediate_output_train[idx]\n",
        "        for idx, ID in enumerate(self.val_id):\n",
        "            val_emb[ID] = intermediate_output_val[idx]\n",
        "        for idx, ID in enumerate(self.test_id):\n",
        "            test_emb[ID] = intermediate_output_test[idx]\n",
        "        pickle.dump([train_emb, val_emb, test_emb],\n",
        "                    open(self.OUTPUT_PATH, \"wb\"))\n",
        "\n",
        "        self.calc_test_result(model.predict(self.test_x),\n",
        "                              self.test_y, self.test_mask)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB8f0K_wRd5C"
      },
      "source": [
        "# Run either train or test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNYNeWcRR6my",
        "outputId": "418c9cd4-2ba7-4f53-ba2c-42545db118ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features  MELD.Features.Models\tmodels\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akEyXrKXQoHU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Setup argument parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.required = True\n",
        "    parser.add_argument(\n",
        "        \"-classify\", help=\"Set the classifiction to be 'Emotion' or 'Sentiment'\", required=True)\n",
        "    parser.add_argument(\n",
        "        \"-modality\", help=\"Set the modality to be 'text' or 'audio' or 'bimodal'\", required=True)\n",
        "    parser.add_argument(\"-train\", default=False,\n",
        "                        action=\"store_true\", help=\"Flag to intiate training\")\n",
        "    parser.add_argument(\"-test\", default=False,\n",
        "                        action=\"store_true\", help=\"Flag to initiate testing\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.classify.lower() not in [\"emotion\", \"sentiment\"]:\n",
        "        print(\"Classification mode hasn't been set properly. Please set the classifiction flag to be: -classify Emotion/Sentiment\")\n",
        "        exit()\n",
        "    if args.modality.lower() not in [\"text\", \"audio\", \"bimodal\"]:\n",
        "        print(\"Modality hasn't been set properly. Please set the modality flag to be: -modality text/audio/bimodal\")\n",
        "        exit()\n",
        "\n",
        "    args.classify = args.classify.title()\n",
        "    args.modality = args.modality.lower()\n",
        "\n",
        "    # Check directory existence\n",
        "    for directory in [\"./data/pickles\", \"./data/models\"]:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "    model = bc_LSTM(args)\n",
        "    model.load_data()\n",
        "\n",
        "    if args.test:\n",
        "        model.test_model()\n",
        "    else:\n",
        "        model.train_model()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}